import requests
from bs4 import BeautifulSoup
from urllib.parse import quote_plus
from datetime import datetime
import firebase_admin
from firebase_admin import credentials, firestore
import time
import os
import sys

# --- [ìˆ˜ì •] íŒŒì¼ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ê³ ì •í•˜ì—¬ ì–´ë””ì„œë“  ì‹¤í–‰ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦ ---
# ì‚¬ìš©ìë‹˜ì˜ í™˜ê²½ì— ë§ì¶˜ ì‹¤ì œ ê²½ë¡œì…ë‹ˆë‹¤.
JSON_PATH = r"c:\Users\gwak\Finance_Final_V2\serviceAccountKey.json"

if not firebase_admin._apps:
    try:
        # íŒŒì¼ì´ ì‹¤ì œë¡œ ìˆëŠ”ì§€ í™•ì¸ë¶€í„° í•©ë‹ˆë‹¤.
        if os.path.exists(JSON_PATH):
            cred = credentials.Certificate(JSON_PATH)
            firebase_admin.initialize_app(cred)
            print("âœ… íŒŒì´ì–´ë² ì´ìŠ¤ ì¸ì¦ ì„±ê³µ")
        else:
            print(f"âŒ ì¸ì¦ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {JSON_PATH}")
            # íŒŒì¼ ìœ„ì¹˜ê°€ ë‹¤ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ í˜„ì¬ ì‹¤í–‰ í´ë”ì˜ íŒŒì¼ì´ë¼ë„ ì‹œë„
            cred = credentials.Certificate("serviceAccountKey.json")
            firebase_admin.initialize_app(cred)
    except Exception as e:
        print(f"âŒ íŒŒì´ì–´ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

db = firestore.client()

# 2. ë©”ì¸ ì‹¤í–‰ ë£¨í”„
print("ğŸš€ í•œêµ­ ë‰´ìŠ¤ ìë™ ì—…ë°ì´íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")


# rs_data/latest ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (ë¦¬ì•¡íŠ¸ ì—°ë™ êµ¬ì¡° ìœ ì§€)
doc = db.collection('rs_data').document('latest').get()
if not doc.exists:
    print("âŒ rs_data/latest ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. 1ë¶„ í›„ ì¬ì‹œë„.")
    sys.exit()

rankings = doc.to_dict().get('rankings', [])
now_str = datetime.now().strftime('%Y-%m-%d %H:%M')
fields_to_add = {}

print(f"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
print(f"ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {now_str}")
print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

for item in rankings:
    code = item['code']
    name = item['name']
    field_key = f"{code}_{name}"
    
    try:
        url = f"https://news.google.com/rss/search?q={quote_plus(name)}&hl=ko&gl=KR&ceid=KR:ko"
        res = requests.get(url, timeout=5)
        soup = BeautifulSoup(res.content, "xml")
        items = soup.find_all("item")[:20]

        articles = []
        for i in items:
            articles.append({
                "title": i.title.text,
                "link": i.link.text,
                "publisher": i.source.text if i.source else "Google News",
                "time": i.pubDate.text
            })

        fields_to_add[field_key] = {
            "update_time": now_str,
            "articles": articles
        }
        print(f" > {name}({code}) ì™„ë£Œ")
        time.sleep(0.3)
    except Exception as e:
        print(f" > {name} ì˜¤ë¥˜: {e}")

# ë¦¬ì•¡íŠ¸ê°€ ì½ëŠ” ë¬¸ì„œì— ë®ì–´ì“°ê¸°
db.collection('stock_news').document('news_kr').set(fields_to_add)


