import requests
from bs4 import BeautifulSoup
from urllib.parse import quote_plus
from datetime import datetime
import firebase_admin
from firebase_admin import credentials, firestore
import time
import os

# 1. íŒŒì´ì–´ë² ì´ìŠ¤ ì´ˆê¸°í™” (ê¹ƒí—ˆë¸Œ/ë¡œì»¬ ê³µìš©)
if not firebase_admin._apps:
    try:
        # 1ìˆœìœ„: ê¹ƒí—ˆë¸Œ ì•¡ì…˜ìš©(í˜„ì¬ í´ë”), 2ìˆœìœ„: ì˜¤ë¹  PC ì ˆëŒ€ ê²½ë¡œ
        if os.path.exists("serviceAccountKey.json"):
            cred = credentials.Certificate("serviceAccountKey.json")
        else:
            cred = credentials.Certificate(r"c:\Users\gwak\Finance_Final_V2\serviceAccountKey.json")
        firebase_admin.initialize_app(cred)
        print("âœ… íŒŒì´ì–´ë² ì´ìŠ¤ ì¸ì¦ ì„±ê³µ")
    except Exception as e:
        print(f"âŒ íŒŒì´ì–´ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

db = firestore.client()

# 2. RS ë°ì´í„°ì—ì„œ ìƒìœ„ ì¢…ëª© ê°€ì ¸ì˜¤ê¸°
doc = db.collection('rs_data').document('latest').get()
if not doc.exists:
    print("âŒ rs_data/latest ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit()

rankings = doc.to_dict().get('rankings', [])
now_str = datetime.now().strftime('%Y-%m-%d %H:%M')
fields_to_add = {}

print(f"ğŸ“° í•œêµ­ ë‰´ìŠ¤ 30ê°œ ìˆ˜ì§‘ ì‹œì‘: {now_str}")

for item in rankings:
    code = item['code']
    name = item['name']
    field_key = f"{code}_{name}"
    
    try:
        # êµ¬ê¸€ ë‰´ìŠ¤ RSS (ê²€ìƒ‰ì–´ ê¸°ë°˜)
        url = f"https://news.google.com/rss/search?q={quote_plus(name)}&hl=ko&gl=KR&ceid=KR:ko"
        res = requests.get(url, timeout=10)
        soup = BeautifulSoup(res.content, "xml")
        items = soup.find_all("item")

        articles = []
        seen_titles = set()
        for i in items:
            # RSS ë‚ ì§œ í˜•ì‹: "Sat, 24 Jan 2026 07:00:00 GMT"
            # ì´ë¥¼ íŒŒì´ì¬ ë‚ ì§œ ê°ì²´ë¡œ ë³€í™˜í•´ì„œ ì •ë ¬ì— ì‚¬ìš©
            title = i.title.text.strip()
            if title in seen_titles:
                continue
            seen_titles.add(title)
            raw_date = i.pubDate.text
            try:
                dt_obj = datetime.strptime(raw_date, '%a, %d %b %Y %H:%M:%S %Z')
            except:
                dt_obj = datetime.now() # ë³€í™˜ ì‹¤íŒ¨ ì‹œ í˜„ì¬ì‹œê°„

            articles.append({
                "title": title,
                "link": i.link.text,
                "publisher": i.source.text if i.source else "Google News",
                "time": dt_obj.strftime('%Y-%m-%d %H:%M'), # ë¦¬ì•¡íŠ¸ì—ì„œ ë³´ê¸° í¸í•œ í˜•ì‹
                "dt_index": dt_obj # ì •ë ¬ìš© ì„ì‹œ í•„ë“œ
            })

        # --- [í•µì‹¬] ìµœì‹ ìˆœ ì •ë ¬ í›„ ìƒìœ„ 30ê°œë§Œ ìë¥´ê¸° ---
        articles.sort(key=lambda x: x['dt_index'], reverse=True)
        final_articles = articles[:20]

        # ì •ë ¬ìš© ì„ì‹œ í•„ë“œ ì‚­ì œ í›„ ì €ì¥
        for a in final_articles: del a['dt_index']

        fields_to_add[field_key] = {
            "update_time": now_str,
            "articles": final_articles
        }
        print(f" > {name}({code}) ìµœì‹  ë‰´ìŠ¤ {len(final_articles)}ê°œ ì™„ë£Œ")
        time.sleep(0.5) # êµ¬ê¸€ ì°¨ë‹¨ ë°©ì§€

    except Exception as e:
        print(f" > {name} ì˜¤ë¥˜: {e}")

# 3. íŒŒì´ì–´ë² ì´ìŠ¤ ì „ì†¡
db.collection('stock_news').document('news_kr').set(fields_to_add)
print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
print("âœ… ëª¨ë“  í•œêµ­ ë‰´ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ")