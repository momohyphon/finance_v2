import requests
from bs4 import BeautifulSoup
from urllib.parse import quote_plus
from datetime import datetime
import firebase_admin
from firebase_admin import credentials, firestore
import time
import os
import sys
import json
import pytz

# 1. íŒŒì´ì–´ë² ì´ìŠ¤ ì¸ì¦ (ê²½ë¡œ ìµœì í™”)
JSON_PATH = r"c:\Users\gwak\Finance_Final_V2\serviceAccountKey.json"
kst = pytz.timezone('Asia/Seoul') 

if not firebase_admin._apps:
    try:
        # 1ìˆœìœ„: ë¡œì»¬ ì ˆëŒ€ ê²½ë¡œ, 2ìˆœìœ„: ê¹ƒí—ˆë¸Œ ì„œë²„(í˜„ì¬ í´ë”)
        if os.path.exists(JSON_PATH):
            cred = credentials.Certificate(JSON_PATH)
            firebase_admin.initialize_app(cred)
            print("âœ… ë¯¸êµ­ë‰´ìŠ¤: ë¡œì»¬ ì¸ì¦ ì„±ê³µ")
        else:
            cred = credentials.Certificate("serviceAccountKey.json")
            firebase_admin.initialize_app(cred)
            print("âœ… ë¯¸êµ­ë‰´ìŠ¤: ê¹ƒí—ˆë¸Œ ì„œë²„ ì¸ì¦ ì„±ê³µ")
    except Exception as e:
        print(f"âŒ íŒŒì´ì–´ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        sys.exit(1)
        
db = firestore.client()

# 2. ë¯¸êµ­ ì£¼ì‹ ë­í‚¹ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì›ë³¸ ê²½ë¡œ ìœ ì§€)
try:
    doc = db.collection('rs_data').document('us_latest').get() 
    if not doc.exists:
        print("âŒ ë¯¸êµ­ ë­í‚¹ ë°ì´í„°(us_latest)ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(0)

    rankings = doc.to_dict().get('rankings', [])
    now_str = datetime.now(kst).strftime('%Y-%m-%d %H:%M')
    fields_to_add = {}

    print(f"ğŸ‡ºğŸ‡¸ ë¯¸êµ­ ë‰´ìŠ¤ ìµœì‹ ìˆœ ê²€ìƒ‰ ì‹œì‘ (í•œêµ­ì‹œê°„ ê¸°ì¤€: {now_str})")

    for item in rankings:
        code = item.get('code') or item.get('ticker')
        name = item['name']
        field_key = f"{code}_{name}"
        
        try:
            url = f"https://news.google.com/rss/search?q={quote_plus(name)}&hl=en-US&gl=US&ceid=US:en"
            res = requests.get(url, timeout=10)
            soup = BeautifulSoup(res.content, "xml")
            items = soup.find_all("item")

            articles = []
            seen_titles = set()
            for i in items:
                title = i.title.text.strip()
                if title in seen_titles:
                    continue
                seen_titles.add(title)
                
                raw_date = i.pubDate.text
                try:
                    dt_obj = datetime.strptime(raw_date, '%a, %d %b %Y %H:%M:%S %Z')
                    dt_obj = dt_obj.replace(tzinfo=pytz.UTC).astimezone(kst)
                except:
                    dt_obj = datetime.now(kst)

                articles.append({
                    "title": title,
                    "link": i.link.text,
                    "publisher": i.source.text if i.source else "Google News",
                    "time": dt_obj.strftime('%Y-%m-%d %H:%M'),
                    "dt_index": dt_obj
                })

            # ğŸ”¥ ì˜¤ë¹  ì›ë³¸ ì •ë ¬ ë¡œì§ ë° 20ê°œ ìœ ì§€
            articles.sort(key=lambda x: x['dt_index'], reverse=True)
            final_articles = articles[:10]

            for a in final_articles: del a['dt_index']

            fields_to_add[field_key] = {
                "update_time": now_str,
                "articles": final_articles
            }
            
            print(f"âœ… {name}({code}) ë‰´ìŠ¤ {len(final_articles)}ê°œ ì™„ë£Œ")
            time.sleep(0.5)

        except Exception as e:
            print(f"âŒ {name} ë‰´ìŠ¤ ì—ëŸ¬: {e}")

    # 3. íŒŒì´ì–´ë² ì´ìŠ¤ ë° ë¡œì»¬ JSON ì €ì¥ (ì›ë³¸ ê²½ë¡œ ê³ ì •)
    if fields_to_add:
        db.collection('stock_news').document('news_us').set(fields_to_add)
        with open('news_us.json', 'w', encoding='utf-8') as f:
            json.dump(fields_to_add, f, ensure_ascii=False, indent=2)
        print(f"ğŸš€ [ì™„ë£Œ] news_us ì—…ë°ì´íŠ¸ ì™„ë£Œ (KST ê¸°ì¤€)")

except Exception as e:
    print(f"âŒ ì „ì²´ ì‹¤í–‰ ì˜¤ë¥˜: {e}")