import requests
from bs4 import BeautifulSoup
from urllib.parse import quote_plus
from datetime import datetime
import firebase_admin
from firebase_admin import credentials, firestore
import time
import os
import sys
import json
import pytz # ğŸ‘ˆ ì‹œê°„ëŒ€ ë³€í™˜ì„ ìœ„í•´ í•„ìˆ˜

# 1. íŒŒì´ì–´ë² ì´ìŠ¤ ì¸ì¦ (ê²½ë¡œ ìµœì í™”)
JSON_PATH = r"c:\Users\gwak\Finance_Final_V2\serviceAccountKey.json"
kst = pytz.timezone('Asia/Seoul') # í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •

if not firebase_admin._apps:
    try:
        if os.path.exists(JSON_PATH):
            cred = credentials.Certificate(JSON_PATH)
            firebase_admin.initialize_app(cred)
            print("âœ… ë¯¸êµ­ë‰´ìŠ¤: ë¡œì»¬ ì¸ì¦ ì„±ê³µ")
        else:
            cred = credentials.Certificate("serviceAccountKey.json")
            firebase_admin.initialize_app(cred)
            print("âœ… ë¯¸êµ­ë‰´ìŠ¤: ê¹ƒí—ˆë¸Œ ì„œë²„ ì¸ì¦ ì„±ê³µ")
    except Exception as e:
        print(f"âŒ íŒŒì´ì–´ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
db = firestore.client()

# 2. ë¯¸êµ­ ì£¼ì‹ ë­í‚¹ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
doc = db.collection('rs_data').document('us_latest').get() 
if not doc.exists:
    print("âŒ ë¯¸êµ­ ë­í‚¹ ë°ì´í„°(us_latest)ê°€ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit()

rankings = doc.to_dict().get('rankings', [])
# ì—…ë°ì´íŠ¸ ì‹œê°„ë„ í•œêµ­ ì‹œê°„ìœ¼ë¡œ ì„¤ì •
now_str = datetime.now(kst).strftime('%Y-%m-%d %H:%M')
fields_to_add = {}

print(f"ğŸ‡ºğŸ‡¸ ë¯¸êµ­ ë‰´ìŠ¤ ìµœì‹ ìˆœ ê²€ìƒ‰ ì‹œì‘ (í•œêµ­ì‹œê°„ ê¸°ì¤€: {now_str})")

for item in rankings:
    code = item.get('code') or item.get('ticker')
    name = item['name']
    field_key = f"{code}_{name}"
    
    try:
        # ë¯¸êµ­ ë‰´ìŠ¤ìš© RSS (ì–¸ì–´: en-US, ì§€ì—­: US)
        url = f"https://news.google.com/rss/search?q={quote_plus(name)}&hl=en-US&gl=US&ceid=US:en"
        res = requests.get(url, timeout=10)
        soup = BeautifulSoup(res.content, "xml")
        items = soup.find_all("item")

        articles = []
        seen_titles = set()
        for i in items:
            title = i.title.text.strip()
            if title in seen_titles:
                continue
            seen_titles.add(title)
            
            raw_date = i.pubDate.text
            try:
                # 1. êµ¬ê¸€ RSS ì‹œê°„(GMT)ì„ íŒŒì´ì¬ ê°ì²´ë¡œ ë³€í™˜
                dt_obj = datetime.strptime(raw_date, '%a, %d %b %Y %H:%M:%S %Z')
                # 2. GMT ì‹œê°„ì„ í•œêµ­ ì‹œê°„(KST)ìœ¼ë¡œ ê°•ì œ ë³€í™˜
                dt_obj = dt_obj.replace(tzinfo=pytz.UTC).astimezone(kst)
            except:
                dt_obj = datetime.now(kst)

            articles.append({
                "title": title,
                "link": i.link.text,
                "publisher": i.source.text if i.source else "Google News",
                "time": dt_obj.strftime('%Y-%m-%d %H:%M'), # í•œêµ­ ì‹œê°„ í¬ë§·
                "dt_index": dt_obj # ì •ë ¬ìš©
            })

        # ğŸ”¥ ìµœì‹ ìˆœ ì •ë ¬ (í•œêµ­ ì‹œê°„ ê¸°ì¤€) í›„ ìƒìœ„ 20ê°œ
        articles.sort(key=lambda x: x['dt_index'], reverse=True)
        final_articles = articles[:20]

        for a in final_articles: del a['dt_index']

        fields_to_add[field_key] = {
            "update_time": now_str,
            "articles": final_articles
        }
        
        print(f"âœ… {name}({code}) ë‰´ìŠ¤ {len(final_articles)}ê°œ ì™„ë£Œ")
        time.sleep(0.5)

    except Exception as e:
        print(f"âŒ {name} ë‰´ìŠ¤ ì—ëŸ¬: {e}")

# 3. íŒŒì´ì–´ë² ì´ìŠ¤ ë° ë¡œì»¬ JSON ì €ì¥
try:
    db.collection('stock_news').document('news_us').set(fields_to_add)
    with open('news_us.json', 'w', encoding='utf-8') as f:
        json.dump(fields_to_add, f, ensure_ascii=False, indent=2)
    print(f"ğŸš€ [ì™„ë£Œ] news_us ì—…ë°ì´íŠ¸ ì™„ë£Œ (KST ê¸°ì¤€)")
except Exception as e:
    print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
